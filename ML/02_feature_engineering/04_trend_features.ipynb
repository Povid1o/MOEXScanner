{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è Global ML Model)\n",
        "\n",
        "**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å** - –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏–∑ `features.trend_features`\n",
        "\n",
        "## –í—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\n",
        "- **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ** —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ MA (dist_to_sma_*, dist_to_ema_*)\n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–∫–ª–æ–Ω—ã MA\n",
        "- Momentum (log return –∑–∞ –ø–µ—Ä–∏–æ–¥)\n",
        "- RSI (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω 0-100)\n",
        "- –°–∏–≥–Ω–∞–ª –∏ —Å–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞\n",
        "\n",
        "**–ü–æ–ª–Ω—ã–π pipeline**: —Å–º. `06_feature_aggregator.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ML –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "from features.trend_features import (\n",
        "    build_trend_features,\n",
        "    TREND_FEATURE_COLUMNS\n",
        ")\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –º–æ–¥—É–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
        "current_dir = Path.cwd()\n",
        "print(f\"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {current_dir}\")\n",
        "\n",
        "# –°—Ç—Ä–æ–∏–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
        "DATA_DIR = current_dir.parent.parent / \"ML\" / \"data\" / \"processed\"\n",
        "OUTPUT_DIR = Path(\"data\") / \"features\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {DATA_DIR}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏\n",
        "if DATA_DIR.exists():\n",
        "    print(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞–π–¥–µ–Ω–∞!\")\n",
        "else:\n",
        "    print(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å\")\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤ –≤–∫–ª—é—á–∞—è –∏–Ω–¥–µ–∫—Å MOEX\n",
        "TICKERS = [\n",
        "    \"AFKS\", \"AFLT\", \"ALRS\", \"BELU\", \"BSPB\", \"CHMF\", \"FIVE\", \"GAZP\", \"GMKN\", \"HYDR\",\n",
        "    \"IMOEX\",  # –ò–Ω–¥–µ–∫—Å –ú–æ—Å–±–∏—Ä–∂–∏\n",
        "    \"IRAO\", \"LENT\", \"LKOH\", \"MAGN\", \"MGNT\", \"MTSS\", \"NLMK\", \"NVTK\", \"OZON\",\n",
        "    \"PIKK\", \"PLZL\", \"ROSN\", \"RTKM\", \"SBER\", \"SNGS\", \"TATN\", \"TCSG\", \"VKCO\", \"VTBR\", \"YNDX\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ –ó–∞–≥—Ä—É–∑–∏–º {len(TICKERS)} —Ç–∏–∫–µ—Ä–æ–≤\")\n",
        "print(f\"–¢–∏–∫–µ—Ä—ã: {TICKERS}\")\n",
        "print(f\"\\n–í—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {TREND_FEATURE_COLUMNS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –†–∞—Å—á–µ—Ç –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ç–∏–∫–µ—Ä—ã\n",
        "results = {}\n",
        "\n",
        "for ticker in TICKERS:\n",
        "    try:\n",
        "        print(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {ticker}...\", end=\" \")\n",
        "        \n",
        "        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        input_path = DATA_DIR / f\"{ticker}_ohlcv_returns.parquet\"\n",
        "        if not input_path.exists():\n",
        "            print(f\"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}\")\n",
        "            continue\n",
        "        \n",
        "        df = pd.read_parquet(input_path)\n",
        "        \n",
        "        # 2. –†–∞—Å—á–µ—Ç —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å\n",
        "        trend_features = build_trend_features(df)\n",
        "        \n",
        "        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "        df_result = pd.concat([df, trend_features], axis=1)\n",
        "        \n",
        "        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        results[ticker] = df_result\n",
        "        print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ ({len(df_result)} —Å—Ç—Ä–æ–∫, {len(trend_features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {ticker}: {e}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} –∏–∑ {len(TICKERS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ü—Ä–∏–º–µ—Ä –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ SBER\n",
        "sample_ticker = \"SBER\"\n",
        "if sample_ticker in results:\n",
        "    df_sample = results[sample_ticker]\n",
        "    \n",
        "    print(f\"–°—Ç–æ–ª–±—Ü—ã: {df_sample.columns.tolist()}\")\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã (–≥–¥–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)\n",
        "    print(f\"\\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –°–ï–†–ï–î–ò–ù–´ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
        "    mid_idx = len(df_sample) // 2\n",
        "    display_cols = [\"date\", \"close\", \"dist_to_sma_20\", \"dist_to_sma_50\", \"momentum_10\", \"rsi_14\", \"trend_signal\", \"trend_strength\"]\n",
        "    print(df_sample[display_cols].iloc[mid_idx:mid_idx+10])\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    print(f\"\\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π:\")\n",
        "    print(df_sample[display_cols].tail(10))\n",
        "    \n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç—Ä–µ–Ω–¥–∞–º\n",
        "    print(f\"\\n--- –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–†–ï–ù–î–û–í ---\")\n",
        "    print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df_sample)}\")\n",
        "    trend_counts = df_sample[\"trend_signal\"].value_counts().sort_index()\n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ —Ç—Ä–µ–Ω–¥–∞:\")\n",
        "    print(f\"  Downtrend (-1): {trend_counts.get(-1, 0)} ({trend_counts.get(-1, 0)/len(df_sample)*100:.1f}%)\")\n",
        "    print(f\"  Sideways (0):   {trend_counts.get(0, 0)} ({trend_counts.get(0, 0)/len(df_sample)*100:.1f}%)\")\n",
        "    print(f\"  Uptrend (1):    {trend_counts.get(1, 0)} ({trend_counts.get(1, 0)/len(df_sample)*100:.1f}%)\")\n",
        "    \n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
        "    print(f\"\\n--- –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í ---\")\n",
        "    print(f\"–°—Ä–µ–¥–Ω–∏–π RSI: {df_sample['rsi_14'].mean():.2f}\")\n",
        "    print(f\"–°—Ä–µ–¥–Ω–∏–π dist_to_sma_50: {df_sample['dist_to_sma_50'].mean():.4f}\")\n",
        "    print(f\"–°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞: {df_sample['trend_strength'].mean():.4f}\")\n",
        "else:\n",
        "    print(f\"–¢–∏–∫–µ—Ä {sample_ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "for ticker, df in results.items():\n",
        "    output_path = OUTPUT_DIR / f\"{ticker}_trend_features.parquet\"\n",
        "    df.to_parquet(output_path, index=False)\n",
        "    print(f\"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}\")\n",
        "\n",
        "print(f\"\\nüéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_DIR}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞: —É–±–µ–¥–∏–º—Å—è —á—Ç–æ –Ω–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö MA\n",
        "forbidden = [\"sma_20\", \"sma_50\", \"sma_200\", \"ema_20\", \"ema_50\"]\n",
        "sample_df = results.get(\"SBER\")\n",
        "if sample_df is not None:\n",
        "    found = [col for col in sample_df.columns if col in forbidden]\n",
        "    if found:\n",
        "        print(f\"\\n‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω—ã –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {found}\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞: –Ω–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö MA –∫–æ–ª–æ–Ω–æ–∫\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
