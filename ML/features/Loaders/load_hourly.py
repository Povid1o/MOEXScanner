"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ —á–∞—Å–æ–≤—ã—Ö (H1) –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–Ω—É—Ç—Ä–∏–¥–Ω–µ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –ó–∞–≥—Ä—É–∑–∫–∏ OHLCV –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–æ–≤ —Å —á–∞—Å–æ–≤—ã–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–º
- –ü–∞—Ä—Å–∏–Ω–≥–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞—Ç
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–Ω—É—Ç—Ä–∏–¥–Ω–µ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

–§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (CSV –∏–∑ MOEX ISS API):
- –°—Ç–æ–ª–±—Ü—ã: open, close, high, low, value, volume, begin, end
- begin/end: datetime –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM-DD HH:MM:SS'

–ê–≤—Ç–æ—Ä: ML Pipeline v2.0 (Intraday Features)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Union, List
import warnings
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_hourly_data(
    ticker: str,
    data_dir: Optional[Union[str, Path]] = None,
    parse_dates: bool = True,
    validate: bool = True
) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞—Å–æ–≤—ã–µ OHLCV –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞.
    
    –§—É–Ω–∫—Ü–∏—è –∏—â–µ—Ç CSV —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data/MOEX_DATA/{ticker}/1H/
    –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'SBER', 'GAZP')
        data_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ MOEX_DATA.
                  –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        parse_dates: –ü–∞—Ä—Å–∏—Ç—å –ª–∏ —Å—Ç–æ–ª–±–µ—Ü 'begin' –∫–∞–∫ datetime
        validate: –ü—Ä–æ–≤–æ–¥–∏—Ç—å –ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        pd.DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
            - datetime (index): –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Å–≤–µ—á–∏
            - open, high, low, close: OHLC —Ü–µ–Ω—ã
            - volume: –û–±—ä–µ–º –≤ –ª–æ—Ç–∞—Ö
            - value: –û–±—ä–µ–º –≤ —Ä—É–±–ª—è—Ö
            - date: –î–∞—Ç–∞ (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏) –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –¥–Ω—è–º
            
    Raises:
        FileNotFoundError: –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        ValueError: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é
        
    Example:
        >>> df = load_hourly_data('SBER')
        >>> print(df.head())
                             open   close    high     low    volume  date
        datetime                                                          
        2024-04-11 10:00:00  306.7  306.85  307.81  306.00  4510230  2024-04-11
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if data_dir is None:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
        current_file = Path(__file__)
        data_dir = current_file.parent.parent.parent / "data" / "MOEX_DATA"
    else:
        data_dir = Path(data_dir)
    
    # –ü—É—Ç—å –∫ —á–∞—Å–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º —Ç–∏–∫–µ—Ä–∞
    hourly_dir = data_dir / ticker / "1H"
    
    if not hourly_dir.exists():
        raise FileNotFoundError(
            f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {hourly_dir}\n"
            f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è —Ç–∏–∫–µ—Ä–∞ {ticker}"
        )
    
    # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    csv_files = list(hourly_dir.glob("*hourly*.csv"))
    
    if not csv_files:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±–æ–π CSV —Ñ–∞–π–ª
        csv_files = list(hourly_dir.glob("*.csv"))
    
    if not csv_files:
        raise FileNotFoundError(
            f"CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {hourly_dir}\n"
            f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç: {ticker}_hourly_*.csv"
        )
    
    # –ë–µ—Ä–µ–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª (–ø–æ –∏–º–µ–Ω–∏)
    csv_path = sorted(csv_files)[-1]
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {csv_path.name}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
    df = pd.read_csv(csv_path)
    
    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
    if parse_dates and 'begin' in df.columns:
        df['datetime'] = pd.to_datetime(df['begin'], errors='coerce')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        null_dates = df['datetime'].isna().sum()
        if null_dates > 0:
            logger.warning(f"‚ö†Ô∏è {null_dates} —Å—Ç—Ä–æ–∫ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏ —É–¥–∞–ª–µ–Ω—ã")
            df = df.dropna(subset=['datetime'])
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º datetime –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
        df = df.set_index('datetime')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–æ–π (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏) –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        df['date'] = df.index.date
        df['date'] = pd.to_datetime(df['date'])
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    df.columns = df.columns.str.lower()
    
    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    cols_to_drop = ['begin', 'end']
    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    df = df.sort_index()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if validate:
        _validate_hourly_data(df, ticker)
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —á–∞—Å–æ–≤—ã—Ö —Å–≤–µ—á–µ–π –¥–ª—è {ticker}")
    logger.info(f"   –ü–µ—Ä–∏–æ–¥: {df.index.min()} - {df.index.max()}")
    
    return df


def _validate_hourly_data(df: pd.DataFrame, ticker: str) -> None:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –ù–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    - –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NaN –≤ —Ü–µ–Ω–∞—Ö
    - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å OHLC (low <= open,close <= high)
    - –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—ä–µ–º–æ–≤
    
    Args:
        df: DataFrame —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        ticker: –¢–∏–∫–µ—Ä –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        
    Raises:
        ValueError: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    """
    required_cols = ['open', 'high', 'low', 'close', 'volume']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        raise ValueError(
            f"[{ticker}] –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {missing_cols}"
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ —Ü–µ–Ω–∞—Ö
    price_cols = ['open', 'high', 'low', 'close']
    nan_counts = df[price_cols].isna().sum()
    
    if nan_counts.sum() > 0:
        logger.warning(f"[{ticker}] NaN –≤ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {nan_counts.to_dict()}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ OHLC constraint: low <= min(open, close) –∏ max(open, close) <= high
    ohlc_violations = (
        (df['low'] > df['open']) | 
        (df['low'] > df['close']) |
        (df['high'] < df['open']) | 
        (df['high'] < df['close'])
    ).sum()
    
    if ohlc_violations > 0:
        logger.warning(f"[{ticker}] ‚ö†Ô∏è {ohlc_violations} —Å–≤–µ—á–µ–π —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º OHLC constraint")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–æ–≤
    negative_volume = (df['volume'] < 0).sum()
    if negative_volume > 0:
        logger.warning(f"[{ticker}] ‚ö†Ô∏è {negative_volume} —Å–≤–µ—á–µ–π —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –æ–±—ä–µ–º–æ–º")


def load_hourly_data_multi(
    tickers: List[str],
    data_dir: Optional[Union[str, Path]] = None
) -> dict:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∏–∫–µ—Ä–æ–≤.
    
    Args:
        tickers: –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤
        data_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
        Dict[ticker, DataFrame] —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    result = {}
    
    for ticker in tickers:
        try:
            result[ticker] = load_hourly_data(ticker, data_dir)
        except FileNotFoundError as e:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {ticker}: {e}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {ticker}: {e}")
    
    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(result)}/{len(tickers)} —Ç–∏–∫–µ—Ä–æ–≤")
    
    return result


def get_trading_hours(df: pd.DataFrame) -> int:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—Ä–≥–æ–≤—ã—Ö —á–∞—Å–æ–≤ –≤ –¥–Ω–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞.
    
    –ù–∞ MOEX –æ—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Å—Å–∏—è: 10:00-18:50 (–æ–∫–æ–ª–æ 9 —á–∞—Å–æ–≤)
    –° —É—á–µ—Ç–æ–º –≤–µ—á–µ—Ä–Ω–µ–π —Å–µ—Å—Å–∏–∏: –¥–æ 23:50 (–æ–∫–æ–ª–æ 14 —á–∞—Å–æ–≤)
    
    Args:
        df: DataFrame —Å —á–∞—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (index = datetime)
        
    Returns:
        –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π –≤ —Ç–æ—Ä–≥–æ–≤–æ–º –¥–Ω–µ
    """
    if 'date' not in df.columns:
        df = df.copy()
        df['date'] = df.index.date
    
    candles_per_day = df.groupby('date').size()
    median_hours = int(candles_per_day.median())
    
    logger.debug(f"–¢–æ—Ä–≥–æ–≤—ã—Ö —á–∞—Å–æ–≤ –≤ –¥–Ω–µ: –º–µ–¥–∏–∞–Ω–∞={median_hours}, min={candles_per_day.min()}, max={candles_per_day.max()}")
    
    return median_hours


# === –≠–ö–°–ü–û–†–¢ ===
__all__ = [
    'load_hourly_data',
    'load_hourly_data_multi',
    'get_trading_hours'
]


if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    print("üß™ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    try:
        df = load_hourly_data('SBER')
        print(f"\nüìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:")
        print(df.head())
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(df[['open', 'high', 'low', 'close', 'volume']].describe())
        
        hours = get_trading_hours(df)
        print(f"\n‚è∞ –¢–æ—Ä–≥–æ–≤—ã—Ö —á–∞—Å–æ–≤ –≤ –¥–Ω–µ: {hours}")
        
    except FileNotFoundError as e:
        print(f"‚ùå {e}")

