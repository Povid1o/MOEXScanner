"""
–ì–ª–æ–±–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å LightGBM –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è:
1. –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ –≤ –µ–¥–∏–Ω—ã–π DataFrame
2. –í—Ä–µ–º–µ–Ω–Ω–æ–π (time-series) split –Ω–∞ train/test
3. –û–±—É—á–µ–Ω–∏–µ 3 –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (0.16, 0.50, 0.84)
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ feature importance

–ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–æ–≥–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π split –±–µ–∑ shuffle!

–ê–≤—Ç–æ—Ä: ML Pipeline v2.0 (Global Model)
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
import warnings
import gc
import json
from datetime import datetime

warnings.filterwarnings('ignore')


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–Ω–µ—à–Ω—é—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
try:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent / "config"))
    from training_config import (
        TRAIN_CUTOFF_DATE as EXT_CUTOFF,
        TARGET_HORIZON as EXT_HORIZON,
        TARGET_COL as EXT_TARGET,
        QUANTILES as EXT_QUANTILES,
        LGBM_PARAMS as EXT_LGBM_PARAMS_BASE,
        NUM_BOOST_ROUND as EXT_NUM_ROUNDS,
        EARLY_STOPPING_ROUNDS as EXT_EARLY_STOP,
        CATEGORICAL_FEATURES as EXT_CAT_FEATURES,
        EXCLUDE_TICKERS as EXT_EXCLUDE_TICKERS,
        get_active_config,
        ACTIVE_PRESET,
    )
    USE_EXTERNAL_CONFIG = True
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞
    preset_config = get_active_config()
    EXT_LGBM_PARAMS = EXT_LGBM_PARAMS_BASE.copy()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø—Ä–µ—Å–µ—Ç–∞
    if 'num_leaves' in preset_config:
        EXT_LGBM_PARAMS['num_leaves'] = preset_config['num_leaves']
    if 'learning_rate' in preset_config:
        EXT_LGBM_PARAMS['learning_rate'] = preset_config['learning_rate']
    if 'lambda_l1' in preset_config:
        EXT_LGBM_PARAMS['lambda_l1'] = preset_config['lambda_l1']
    if 'lambda_l2' in preset_config:
        EXT_LGBM_PARAMS['lambda_l2'] = preset_config['lambda_l2']
    if 'min_child_samples' in preset_config:
        EXT_LGBM_PARAMS['min_child_samples'] = preset_config['min_child_samples']
    
    # –û–±–Ω–æ–≤–ª—è–µ–º cutoff –∏–∑ –ø—Ä–µ—Å–µ—Ç–∞
    if 'train_cutoff' in preset_config:
        EXT_CUTOFF = preset_config['train_cutoff']
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤–Ω–µ—à–Ω—è—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ config/training_config.py")
    print(f"üìå –ê–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–µ—Å–µ—Ç: {ACTIVE_PRESET}")
    print(f"üìÖ Train cutoff: {EXT_CUTOFF}")
except ImportError as e:
    USE_EXTERNAL_CONFIG = False
    EXT_LGBM_PARAMS = None
    print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e})")


class Config:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    
    üí° –î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ: config/training_config.py
    """
    
    # –ü—É—Ç–∏
    ML_ROOT = Path(__file__).parent.parent
    DATA_DIR = ML_ROOT / "data" / "processed_ml"
    OUTPUT_MODEL_DIR = ML_ROOT / "data" / "models"
    REPORTS_DIR = ML_ROOT / "reports"
    
    # === –ü–ê–†–ê–ú–ï–¢–†–´ –ò–ó –í–ù–ï–®–ù–ï–ô –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò (–∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ) ===
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π split
    TRAIN_CUTOFF_DATE = EXT_CUTOFF if USE_EXTERNAL_CONFIG else '2024-01-01'
    
    # –ö–≤–∞–Ω—Ç–∏–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (–≥—Ä–∞–Ω–∏—Ü—ã 1-sigma –∏ –º–µ–¥–∏–∞–Ω–∞)
    QUANTILES = EXT_QUANTILES if USE_EXTERNAL_CONFIG else [0.16, 0.50, 0.84]
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–±—É–¥—É—â–∞—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
    TARGET_HORIZON = EXT_HORIZON if USE_EXTERNAL_CONFIG else 5
    TARGET_COL = EXT_TARGET if USE_EXTERNAL_CONFIG else 'target_vol_5d'
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    CATEGORICAL_FEATURES = EXT_CAT_FEATURES if USE_EXTERNAL_CONFIG else [
        'ticker_id', 
        'sector_id',
        'is_month_end',
        'is_month_start',
        'day_of_week',
        'vp_above_va',
        'volume_spike',
        'trend_signal',
        'price_position_ma'
    ]
    
    # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ —Ç–∏–∫–µ—Ä—ã
    EXCLUDE_TICKERS = EXT_EXCLUDE_TICKERS if USE_EXTERNAL_CONFIG else []
    
    # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    EXCLUDE_COLS = ['date', TARGET_COL, 'ticker_id', 'sector_id']
    
    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM
    LGBM_PARAMS = EXT_LGBM_PARAMS if USE_EXTERNAL_CONFIG else {
        'boosting_type': 'gbdt',
        'objective': 'quantile',
        'metric': 'quantile',
        'num_leaves': 63,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'lambda_l1': 0.1,
        'lambda_l2': 0.1,
        'min_child_samples': 20,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    NUM_BOOST_ROUND = EXT_NUM_ROUNDS if USE_EXTERNAL_CONFIG else 1000
    EARLY_STOPPING_ROUNDS = EXT_EARLY_STOP if USE_EXTERNAL_CONFIG else 50


# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ì–†–ï–ì–ê–¶–ò–Ø –î–ê–ù–ù–´–•
# ============================================================================

def load_all_ticker_data(data_dir: Path) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã *_ml_features.parquet –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω DataFrame.
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - Downcast float64 -> float32 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    - –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ticker_id –∏ sector_id –≤ category
    
    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å parquet —Ñ–∞–π–ª–∞–º–∏
        
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤
    """
    print("=" * 60)
    print("üì• –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ì–†–ï–ì–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
    files = list(data_dir.glob("*_ml_features.parquet"))
    
    if not files:
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –≤ {data_dir}")
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä–æ–≤ –¥–ª—è liquidity_rank
    metadata_path = data_dir.parent.parent / "config" / "tickers_metadata.json"
    ticker_metadata = {}
    if metadata_path.exists():
        with open(metadata_path, 'r', encoding='utf-8') as f:
            ticker_metadata = json.load(f)
        print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ {metadata_path.name}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã (—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤)
    dfs = []
    excluded_count = 0
    
    for f in files:
        ticker = f.stem.replace('_ml_features', '')
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã
        if ticker in Config.EXCLUDE_TICKERS:
            print(f"   ‚è≠Ô∏è {ticker}: –ò–°–ö–õ–Æ–ß–Å–ù –∏–∑ –æ–±—É—á–µ–Ω–∏—è")
            excluded_count += 1
            continue
        
        df = pd.read_parquet(f)
        
        # –î–æ–±–∞–≤–ª—è–µ–º liquidity_rank –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö
        if 'liquidity_rank' not in df.columns and ticker in ticker_metadata:
            df['liquidity_rank'] = ticker_metadata[ticker].get('liquidity_rank', 30)
        
        print(f"   ‚Ä¢ {ticker}: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        dfs.append(df)
    
    if excluded_count:
        print(f"\n‚ö†Ô∏è –ò—Å–∫–ª—é—á–µ–Ω–æ —Ç–∏–∫–µ—Ä–æ–≤: {excluded_count}")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω DataFrame
    global_df = pd.concat(dfs, ignore_index=True)
    print(f"\nüìä –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(global_df):,} —Å—Ç—Ä–æ–∫")
    
    # –ü–∞–º—è—Ç—å –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    mem_before = global_df.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"üíæ –ü–∞–º—è—Ç—å –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {mem_before:.1f} MB")
    
    # === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–ú–Ø–¢–ò ===
    
    # Downcast float64 -> float32
    float_cols = global_df.select_dtypes(include=['float64']).columns
    for col in float_cols:
        global_df[col] = global_df[col].astype('float32')
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if 'ticker_id' in global_df.columns:
        global_df['ticker_id'] = global_df['ticker_id'].astype('category')
    
    if 'sector_id' in global_df.columns:
        global_df['sector_id'] = global_df['sector_id'].astype('category')
    
    # –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    mem_after = global_df.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {mem_after:.1f} MB ({(1 - mem_after/mem_before)*100:.1f}% —ç–∫–æ–Ω–æ–º–∏—è)")
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    del dfs
    gc.collect()
    
    return global_df


def create_target_variable(df: pd.DataFrame, horizon: int = 5) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞—ë—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é: —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ.
    
    –§–æ—Ä–º—É–ª–∞: rolling_std(log_return, horizon).shift(-horizon) * sqrt(252)
    
    –ö–†–ò–¢–ò–ß–ù–û: shift(-horizon) —Å–º–µ—â–∞–µ—Ç –Ω–∞ –±—É–¥—É—â–µ–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è!
    
    Args:
        df: DataFrame —Å log_return
        horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ –¥–Ω—è—Ö
        
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    """
    print(f"\nüéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–≥–æ—Ä–∏–∑–æ–Ω—Ç: {horizon} –¥–Ω–µ–π)...")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á—ë—Ç–∞
    df = df.sort_values(['ticker_id', 'date'])
    
    def calc_target(group):
        """–†–∞—Å—á—ë—Ç target –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞."""
        future_vol = (
            group['log_return']
            .rolling(window=horizon)
            .std()
            .shift(-horizon)  # –°–º–µ—â–∞–µ–º –Ω–∞ –±—É–¥—É—â–µ–µ!
            * np.sqrt(252)    # –ê–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏—è
        )
        return future_vol
    
    df[Config.TARGET_COL] = df.groupby('ticker_id', observed=True).apply(
        lambda x: calc_target(x)
    ).reset_index(level=0, drop=True)
    
    print(f"   ‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–∑–¥–∞–Ω–∞: {Config.TARGET_COL}")
    print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ target: mean={df[Config.TARGET_COL].mean():.4f}, "
          f"std={df[Config.TARGET_COL].std():.4f}")
    
    return df


# ============================================================================
# 2. –í–†–ï–ú–ï–ù–ù–û–ô TRAIN/TEST SPLIT
# ============================================================================

def time_series_split(
    df: pd.DataFrame, 
    cutoff_date: str
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –°—Ç—Ä–æ–≥–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π split –ë–ï–ó shuffle.
    
    –ö–†–ò–¢–ò–ß–ù–û: –ù–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å random split - —ç—Ç–æ –ø—Ä–∏–≤–µ–¥—ë—Ç –∫ look-ahead bias!
    
    Args:
        df: –ü–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        cutoff_date: –î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è (—Ñ–æ—Ä–º–∞—Ç 'YYYY-MM-DD')
        
    Returns:
        Tuple[train_df, test_df]
    """
    print("\n" + "=" * 60)
    print("‚úÇÔ∏è –≠–¢–ê–ü 2: –í–†–ï–ú–ï–ù–ù–û–ô SPLIT (Time Series)")
    print("=" * 60)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
    df['date'] = pd.to_datetime(df['date'])
    cutoff = pd.Timestamp(cutoff_date)
    
    # Split –ø–æ –¥–∞—Ç–µ
    train_df = df[df['date'] < cutoff].copy()
    test_df = df[df['date'] >= cutoff].copy()
    
    print(f"üìÖ Cutoff –¥–∞—Ç–∞: {cutoff_date}")
    print(f"üìà Train: {len(train_df):,} —Å—Ç—Ä–æ–∫ ({train_df['date'].min().date()} - {train_df['date'].max().date()})")
    print(f"üìâ Test:  {len(test_df):,} —Å—Ç—Ä–æ–∫ ({test_df['date'].min().date()} - {test_df['date'].max().date()})")
    print(f"üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(train_df)/(len(train_df)+len(test_df))*100:.1f}% / "
          f"{len(test_df)/(len(train_df)+len(test_df))*100:.1f}%")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã
    train_tickers = set(train_df['ticker_id'].unique())
    test_tickers = set(test_df['ticker_id'].unique())
    print(f"üè∑Ô∏è –¢–∏–∫–µ—Ä–æ–≤ –≤ train: {len(train_tickers)}, –≤ test: {len(test_tickers)}")
    
    if test_tickers - train_tickers:
        print(f"‚ö†Ô∏è –¢–∏–∫–µ—Ä—ã —Ç–æ–ª—å–∫–æ –≤ test: {test_tickers - train_tickers}")
    
    return train_df, test_df


# ============================================================================
# 3. SAMPLE WEIGHTING (–ø–æ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏)
# ============================================================================

def create_sample_weights(df: pd.DataFrame) -> np.ndarray:
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ—Å–∞ —Å—ç–º–ø–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏.
    
    –§–æ—Ä–º—É–ª–∞: weight = 1 / log(liquidity_rank + 1)
    
    –ë–æ–ª–µ–µ –ª–∏–∫–≤–∏–¥–Ω—ã–µ –∞–∫—Ç–∏–≤—ã (–º–µ–Ω—å—à–∏–π rank) –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å,
    —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∏—Ä–æ–≤–∞–ª–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    
    Args:
        df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π liquidity_rank
        
    Returns:
        np.ndarray —Å –≤–µ—Å–∞–º–∏
    """
    if 'liquidity_rank' not in df.columns or df['liquidity_rank'].isna().all():
        print("‚ö†Ô∏è liquidity_rank –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞")
        return np.ones(len(df))
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –º–µ–¥–∏–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
    liquidity = df['liquidity_rank'].fillna(df['liquidity_rank'].median()).values
    
    # –§–æ—Ä–º—É–ª–∞ –≤–µ—Å–æ–≤: –±–æ–ª–µ–µ –ª–∏–∫–≤–∏–¥–Ω—ã–µ (rank –±–ª–∏–∂–µ –∫ 1) –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å
    weights = 1.0 / np.log(liquidity + 2)  # +2 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å log(1)=0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    weights = weights / weights.mean()
    
    print(f"‚öñÔ∏è Sample weights: min={weights.min():.3f}, max={weights.max():.3f}, mean={weights.mean():.3f}")
    
    return weights


# ============================================================================
# 4. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LIGHTGBM
# ============================================================================

def prepare_lgbm_data(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    target_col: str,
    categorical_features: List[str]
) -> Dict:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM.
    
    Args:
        train_df: –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        test_df: –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        target_col: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        categorical_features: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
    Returns:
        Dict —Å X_train, y_train, X_test, y_test, feature_names, cat_features
    """
    print("\n" + "=" * 60)
    print("üîß –≠–¢–ê–ü 3: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ target
    train_df = train_df.dropna(subset=[target_col])
    test_df = test_df.dropna(subset=[target_col])
    
    print(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN: train={len(train_df):,}, test={len(test_df):,}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã)
    exclude_cols = ['date', target_col]
    feature_cols = [col for col in train_df.columns if col not in exclude_cols]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö)
    cat_features = [col for col in categorical_features if col in feature_cols]
    
    print(f"üìã –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    print(f"üìã –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(cat_features)}")
    print(f"   {cat_features}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y
    X_train = train_df[feature_cols].copy()
    y_train = train_df[target_col].values
    
    X_test = test_df[feature_cols].copy()
    y_test = test_df[target_col].values
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –≤ –∫–æ–¥—ã –¥–ª—è LightGBM
    for col in cat_features:
        if col in X_train.columns:
            # –ï—Å–ª–∏ –Ω–µ category - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
            if X_train[col].dtype.name != 'category':
                X_train[col] = X_train[col].astype('category')
            if X_test[col].dtype.name != 'category':
                X_test[col] = X_test[col].astype('category')
    
    # Sample weights –¥–ª—è train
    sample_weights = create_sample_weights(train_df)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Ç–æ–ª—å–∫–æ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
    # LightGBM —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å NaN –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns
    X_train[numeric_cols] = X_train[numeric_cols].fillna(0)
    X_test[numeric_cols] = X_test[numeric_cols].fillna(0)
    
    return {
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'feature_names': feature_cols,
        'cat_features': cat_features,
        'sample_weights': sample_weights
    }


# ============================================================================
# 5. –û–ë–£–ß–ï–ù–ò–ï –ö–í–ê–ù–¢–ò–õ–¨–ù–´–• –ú–û–î–ï–õ–ï–ô
# ============================================================================

def train_quantile_models(
    data: Dict,
    quantiles: List[float],
    params: Dict,
    num_boost_round: int,
    early_stopping_rounds: int
) -> Dict[float, lgb.Booster]:
    """
    –û–±—É—á–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–≤–∞–Ω—Ç–∏–ª—è.
    
    Args:
        data: Dict —Å X_train, y_train, X_test, y_test
        quantiles: –°–ø–∏—Å–æ–∫ –∫–≤–∞–Ω—Ç–∏–ª–µ–π [0.16, 0.50, 0.84]
        params: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM
        num_boost_round: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
        early_stopping_rounds: Early stopping
        
    Returns:
        Dict[alpha, lgb.Booster]: –°–ª–æ–≤–∞—Ä—å –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    """
    print("\n" + "=" * 60)
    print("üöÄ –≠–¢–ê–ü 4: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º LightGBM datasets
    train_data = lgb.Dataset(
        data['X_train'],
        label=data['y_train'],
        weight=data['sample_weights'],
        categorical_feature=data['cat_features'],
        free_raw_data=False
    )
    
    valid_data = lgb.Dataset(
        data['X_test'],
        label=data['y_test'],
        categorical_feature=data['cat_features'],
        reference=train_data,
        free_raw_data=False
    )
    
    models = {}
    
    for alpha in quantiles:
        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–≤–∞–Ω—Ç–∏–ª—è Œ±={alpha}...")
        
        # –ö–æ–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–≤–∞–Ω—Ç–∏–ª—å
        model_params = params.copy()
        model_params['alpha'] = alpha
        
        # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
        model = lgb.train(
            model_params,
            train_data,
            num_boost_round=num_boost_round,
            valid_sets=[train_data, valid_data],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(stopping_rounds=early_stopping_rounds),
                lgb.log_evaluation(period=100)
            ]
        )
        
        models[alpha] = model
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        train_pred = model.predict(data['X_train'])
        test_pred = model.predict(data['X_test'])
        
        train_loss = quantile_loss(data['y_train'], train_pred, alpha)
        test_loss = quantile_loss(data['y_test'], test_pred, alpha)
        
        print(f"   üìà Train Quantile Loss: {train_loss:.6f}")
        print(f"   üìâ Test Quantile Loss:  {test_loss:.6f}")
        print(f"   üå≥ Best iteration: {model.best_iteration}")
    
    return models


def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, alpha: float) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Quantile Loss (Pinball Loss).
    
    –§–æ—Ä–º—É–ª–∞: mean(max(alpha*(y-pred), (alpha-1)*(y-pred)))
    """
    residual = y_true - y_pred
    loss = np.where(residual >= 0, alpha * residual, (alpha - 1) * residual)
    return np.mean(loss)


# ============================================================================
# 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
# ============================================================================

def save_models(
    models: Dict[float, lgb.Booster],
    output_dir: Path
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª—ã.
    
    Args:
        models: –°–ª–æ–≤–∞—Ä—å {alpha: model}
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    print("\n" + "=" * 60)
    print("üíæ –≠–¢–ê–ü 5: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for alpha, model in models.items():
        # –§–æ—Ä–º–∞—Ç: global_lgbm_q16.txt, global_lgbm_q50.txt, global_lgbm_q84.txt
        filename = f"global_lgbm_q{int(alpha*100)}.txt"
        path = output_dir / filename
        model.save_model(str(path))
        print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {path}")


# ============================================================================
# 7. FEATURE IMPORTANCE
# ============================================================================

def plot_feature_importance(
    models: Dict[float, lgb.Booster],
    output_dir: Path,
    top_n: int = 30
) -> None:
    """
    –°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    Args:
        models: –°–ª–æ–≤–∞—Ä—å –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    print("\n" + "=" * 60)
    print("üìä –≠–¢–ê–ü 6: FEATURE IMPORTANCE")
    print("=" * 60)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (q50) –¥–ª—è importance
    model = models[0.50]
    
    # –ü–æ–ª—É—á–∞–µ–º importance (–ø–æ gain)
    importance = model.feature_importance(importance_type='gain')
    feature_names = model.feature_name()
    
    # –°–æ–∑–¥–∞—ë–º DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    imp_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    print(f"\nüèÜ –¢–æ–ø-{top_n} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (gain):")
    print(imp_df.head(top_n).to_string(index=False))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    csv_path = output_dir / "feature_importance.csv"
    imp_df.to_csv(csv_path, index=False)
    print(f"\nüíæ CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {csv_path}")
    
    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫
    fig, ax = plt.subplots(figsize=(12, 10))
    
    top_features = imp_df.head(top_n)
    y_pos = np.arange(len(top_features))
    
    bars = ax.barh(y_pos, top_features['importance'], color='steelblue', edgecolor='navy')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(top_features['feature'])
    ax.invert_yaxis()  # –¢–æ–ø —Å–≤–µ—Ä—Ö—É
    ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
    ax.set_title('LightGBM Global Model - Feature Importance', fontsize=14, fontweight='bold')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä–∞—Ö
    for bar, val in zip(bars, top_features['importance']):
        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                f'{val:.1f}', va='center', fontsize=9)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plot_path = output_dir / "feature_importance.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"üìà –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {plot_path}")


# ============================================================================
# 8. –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –û–¢–ß–Å–¢
# ============================================================================

def generate_validation_report(
    models: Dict[float, lgb.Booster],
    data: Dict,
    output_dir: Path
) -> None:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏.
    
    Args:
        models: –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –æ—Ç—á—ë—Ç–∞
    """
    print("\n" + "=" * 60)
    print("üìã –û–¢–ß–Å–¢ –û –í–ê–õ–ò–î–ê–¶–ò–ò")
    print("=" * 60)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ü—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–≤–∞–Ω—Ç–∏–ª—è
    predictions = {}
    for alpha, model in models.items():
        predictions[alpha] = model.predict(data['X_test'])
    
    y_true = data['y_test']
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ [q16, q84]
    lower = predictions[0.16]
    upper = predictions[0.84]
    
    coverage = np.mean((y_true >= lower) & (y_true <= upper))
    expected_coverage = 0.84 - 0.16  # 68%
    
    print(f"\nüéØ –ü–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ [q16, q84]:")
    print(f"   –û–∂–∏–¥–∞–µ–º–æ–µ: {expected_coverage*100:.1f}%")
    print(f"   –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ: {coverage*100:.1f}%")
    
    # –°—Ä–µ–¥–Ω—è—è —à–∏—Ä–∏–Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    interval_width = np.mean(upper - lower)
    print(f"\nüìè –°—Ä–µ–¥–Ω—è—è —à–∏—Ä–∏–Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞: {interval_width:.4f}")
    
    # Quantile losses
    print(f"\nüìä Quantile Loss –Ω–∞ —Ç–µ—Å—Ç–µ:")
    for alpha in [0.16, 0.50, 0.84]:
        loss = quantile_loss(y_true, predictions[alpha], alpha)
        print(f"   Œ±={alpha}: {loss:.6f}")
    
    # MAE –¥–ª—è –º–µ–¥–∏–∞–Ω—ã
    mae = np.mean(np.abs(y_true - predictions[0.50]))
    print(f"\nüìê MAE (–º–µ–¥–∏–∞–Ω–∞ q50): {mae:.6f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report = {
        'cutoff_date': Config.TRAIN_CUTOFF_DATE,
        'train_samples': len(data['y_train']),
        'test_samples': len(data['y_test']),
        'coverage_68': coverage,
        'interval_width': interval_width,
        'mae_median': mae,
        'quantile_loss_16': quantile_loss(y_true, predictions[0.16], 0.16),
        'quantile_loss_50': quantile_loss(y_true, predictions[0.50], 0.50),
        'quantile_loss_84': quantile_loss(y_true, predictions[0.84], 0.84),
        'timestamp': datetime.now().isoformat()
    }
    
    report_df = pd.DataFrame([report])
    report_path = output_dir / "validation_report.csv"
    report_df.to_csv(report_path, index=False)
    print(f"\nüíæ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")


# ============================================================================
# MAIN
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è."""
    
    print("\n" + "=" * 60)
    print("üöÄ GLOBAL LIGHTGBM MODEL TRAINING PIPELINE")
    print("=" * 60)
    print(f"üìÖ –ó–∞–ø—É—Å–∫: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {Config.TARGET_COL}")
    print(f"üìä –ö–≤–∞–Ω—Ç–∏–ª–∏: {Config.QUANTILES}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    global_df = load_all_ticker_data(Config.DATA_DIR)
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    global_df = create_target_variable(global_df, horizon=Config.TARGET_HORIZON)
    
    # 3. –í—Ä–µ–º–µ–Ω–Ω–æ–π split
    train_df, test_df = time_series_split(global_df, Config.TRAIN_CUTOFF_DATE)
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
    del global_df
    gc.collect()
    
    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LightGBM
    data = prepare_lgbm_data(
        train_df, 
        test_df, 
        Config.TARGET_COL,
        Config.CATEGORICAL_FEATURES
    )
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
    del train_df, test_df
    gc.collect()
    
    # 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models = train_quantile_models(
        data,
        Config.QUANTILES,
        Config.LGBM_PARAMS,
        Config.NUM_BOOST_ROUND,
        Config.EARLY_STOPPING_ROUNDS
    )
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    save_models(models, Config.OUTPUT_MODEL_DIR)
    
    # 7. Feature Importance
    plot_feature_importance(models, Config.REPORTS_DIR, top_n=30)
    
    # 8. –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç
    generate_validation_report(models, data, Config.REPORTS_DIR)
    
    print("\n" + "=" * 60)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 60)
    print(f"\nüìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {Config.OUTPUT_MODEL_DIR}")
    print(f"üìÅ –û—Ç—á—ë—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {Config.REPORTS_DIR}")


if __name__ == "__main__":
    main()

