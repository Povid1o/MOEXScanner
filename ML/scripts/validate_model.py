"""
üî¨ –°–∫—Ä–∏–ø—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –ö–∞–ª–∏–±—Ä–æ–≤–∫—É –∫–≤–∞–Ω—Ç–∏–ª–µ–π (Coverage)
2. –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (Accuracy)
3. –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–∫–µ—Ä–∞–º (Consistency)
4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
"""

import numpy as np
import pandas as pd
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

ML_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ML_ROOT))
sys.path.insert(0, str(ML_ROOT / "03_models"))

from inference import GlobalQuantileModel


def calculate_coverage(df: pd.DataFrame, q_low: float = 0.16, q_high: float = 0.84) -> dict:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –∫–≤–∞–Ω—Ç–∏–ª–µ–π.
    
    –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏: 68% —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–æ–ª–∂–Ω—ã –ø–æ–ø–∞—Å—Ç—å –≤ [q16, q84]
    """
    in_interval = (df['actual'] >= df['pred_q16']) & (df['actual'] <= df['pred_q84'])
    coverage = in_interval.mean()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥–æ–≥–æ –∫–≤–∞–Ω—Ç–∏–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ
    below_q16 = (df['actual'] < df['pred_q16']).mean()
    above_q84 = (df['actual'] > df['pred_q84']).mean()
    
    return {
        'coverage_68': coverage,
        'expected_coverage': q_high - q_low,
        'below_q16': below_q16,  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~16%
        'above_q84': above_q84,  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~16%
        'calibration_error': abs(coverage - (q_high - q_low))
    }


def calculate_accuracy_metrics(df: pd.DataFrame) -> dict:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤."""
    errors = df['actual'] - df['pred_q50']
    
    return {
        'mae': np.abs(errors).mean(),
        'rmse': np.sqrt((errors ** 2).mean()),
        'mape': (np.abs(errors) / df['actual'].replace(0, np.nan)).mean(),
        'bias': errors.mean(),  # —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
        'correlation': df['actual'].corr(df['pred_q50'])
    }


def load_and_validate(ticker: str = None) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    
    model = GlobalQuantileModel()
    model.load_models()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_dir = ML_ROOT / "data" / "processed_ml"
    
    if ticker:
        files = [data_dir / f"{ticker}_ml_features.parquet"]
    else:
        files = list(data_dir.glob("*_ml_features.parquet"))
    
    all_results = []
    
    for file in files:
        if not file.exists():
            continue
            
        ticker_name = file.stem.replace('_ml_features', '')
        df = pd.read_parquet(file)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º IMOEX (–∏–Ω–¥–µ–∫—Å)
        if ticker_name == 'IMOEX':
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
        predictions = model.predict(df, return_interval=True)
        
        # –°–æ–∑–¥–∞—ë–º —Ç–∞—Ä–≥–µ—Ç - —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å
        # target = volatility_{t+1} (forward looking)
        if 'rv_5d' in df.columns:
            df['actual'] = df['rv_5d'].shift(-5)  # 5-–¥–Ω–µ–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        elif 'parkinson_vol_10d' in df.columns:
            df['actual'] = df['parkinson_vol_10d'].shift(-10)
        else:
            df['actual'] = df['ewma_vol_20d'].shift(-1) if 'ewma_vol_20d' in df.columns else np.nan
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        result = pd.concat([
            df[['date', 'actual']].reset_index(drop=True),
            predictions.reset_index(drop=True)
        ], axis=1)
        
        result['ticker'] = ticker_name
        result = result.dropna(subset=['actual', 'pred_q50'])
        
        all_results.append(result)
    
    return pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()


def run_validation():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏."""
    
    print("=" * 70)
    print("üî¨ –í–ê–õ–ò–î–ê–¶–ò–Ø –ú–û–î–ï–õ–ò –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–ò")
    print("=" * 70)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤...")
    df = load_and_validate()
    
    if df.empty:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏!")
        return
    
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} –∑–∞–ø–∏—Å–µ–π –ø–æ {df['ticker'].nunique()} —Ç–∏–∫–µ—Ä–∞–º")
    
    # 2. –û–±—â–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞
    print("\n" + "=" * 70)
    print("üìà –ü–†–û–í–ï–†–ö–ê –ö–ê–õ–ò–ë–†–û–í–ö–ò –ö–í–ê–ù–¢–ò–õ–ï–ô")
    print("=" * 70)
    
    coverage = calculate_coverage(df)
    
    print(f"\n   –ü–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ [q16, q84]:")
    print(f"      –û–∂–∏–¥–∞–µ–º–æ–µ: {coverage['expected_coverage']:.1%}")
    print(f"      –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ: {coverage['coverage_68']:.1%}")
    print(f"      –û—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {coverage['calibration_error']:.1%}")
    
    print(f"\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ö–≤–æ—Å—Ç–æ–≤:")
    print(f"      –ù–∏–∂–µ q16: {coverage['below_q16']:.1%} (–æ–∂–∏–¥–∞–µ—Ç—Å—è ~16%)")
    print(f"      –í—ã—à–µ q84: {coverage['above_q84']:.1%} (–æ–∂–∏–¥–∞–µ—Ç—Å—è ~16%)")
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    if coverage['calibration_error'] < 0.05:
        print("\n   ‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –û–¢–õ–ò–ß–ù–ê–Ø (–æ—à–∏–±–∫–∞ <5%)")
    elif coverage['calibration_error'] < 0.10:
        print("\n   ‚úì –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –•–û–†–û–®–ê–Ø (–æ—à–∏–±–∫–∞ <10%)")
    else:
        print("\n   ‚ö†Ô∏è –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø (–æ—à–∏–±–∫–∞ >10%)")
    
    # 3. –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
    print("\n" + "=" * 70)
    print("üìè –ú–ï–¢–†–ò–ö–ò –¢–û–ß–ù–û–°–¢–ò")
    print("=" * 70)
    
    accuracy = calculate_accuracy_metrics(df)
    
    print(f"\n   MAE (Mean Absolute Error): {accuracy['mae']:.4f}")
    print(f"   RMSE: {accuracy['rmse']:.4f}")
    print(f"   MAPE: {accuracy['mape']:.1%}")
    print(f"   Bias (—Å–º–µ—â–µ–Ω–∏–µ): {accuracy['bias']:.4f}")
    print(f"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {accuracy['correlation']:.3f}")
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
    if accuracy['correlation'] > 0.7:
        print("\n   ‚úÖ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –í–´–°–û–ö–ê–Ø - –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã")
    elif accuracy['correlation'] > 0.5:
        print("\n   ‚úì –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –£–ú–ï–†–ï–ù–ù–ê–Ø - –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —É–ª—É—á—à–µ–Ω–∏—è")
    else:
        print("\n   ‚ö†Ô∏è –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ù–ò–ó–ö–ê–Ø - –º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
    
    if abs(accuracy['bias']) > 0.02:
        direction = "–ó–ê–í–´–®–ê–ï–¢" if accuracy['bias'] > 0 else "–ó–ê–ù–ò–ñ–ê–ï–¢"
        print(f"   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ {direction} –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ {abs(accuracy['bias']):.2%}")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–∫–µ—Ä–∞–º
    print("\n" + "=" * 70)
    print("üìä –ê–ù–ê–õ–ò–ó –ü–û –¢–ò–ö–ï–†–ê–ú")
    print("=" * 70)
    
    ticker_stats = []
    for ticker in df['ticker'].unique():
        ticker_df = df[df['ticker'] == ticker]
        cov = calculate_coverage(ticker_df)
        acc = calculate_accuracy_metrics(ticker_df)
        
        ticker_stats.append({
            'ticker': ticker,
            'n_samples': len(ticker_df),
            'coverage': cov['coverage_68'],
            'mae': acc['mae'],
            'correlation': acc['correlation']
        })
    
    stats_df = pd.DataFrame(ticker_stats).sort_values('correlation', ascending=False)
    
    print(f"\n   {'–¢–∏–∫–µ—Ä':<8} {'N':<8} {'Coverage':<10} {'MAE':<10} {'Corr':<10}")
    print("   " + "-" * 46)
    
    for _, row in stats_df.head(15).iterrows():
        cov_status = "‚úì" if abs(row['coverage'] - 0.68) < 0.1 else "‚ö†"
        print(f"   {row['ticker']:<8} {row['n_samples']:<8} {row['coverage']:.1%} {cov_status:<3} {row['mae']:.4f}    {row['correlation']:.3f}")
    
    # 5. –•—É–¥—à–∏–µ/–ª—É—á—à–∏–µ —Ç–∏–∫–µ—Ä—ã
    print("\n" + "=" * 70)
    print("üèÜ –õ–£–ß–®–ò–ï/–•–£–î–®–ò–ï –¢–ò–ö–ï–†–´")
    print("=" * 70)
    
    print("\n   –¢–æ–ø-5 –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:")
    for _, row in stats_df.head(5).iterrows():
        print(f"      {row['ticker']}: r={row['correlation']:.3f}")
    
    print("\n   –•—É–¥—à–∏–µ 5 –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:")
    for _, row in stats_df.tail(5).iterrows():
        print(f"      {row['ticker']}: r={row['correlation']:.3f}")
    
    # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ look-ahead bias
    print("\n" + "=" * 70)
    print("üîç –ü–†–û–í–ï–†–ö–ê –ù–ê LOOK-AHEAD BIAS")
    print("=" * 70)
    
    # –ï—Å–ª–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è - –≤–æ–∑–º–æ–∂–µ–Ω data leakage
    if accuracy['correlation'] > 0.95:
        print("\n   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è!")
        print("      –í–æ–∑–º–æ–∂–µ–Ω look-ahead bias –∏–ª–∏ data leakage")
    else:
        print("\n   ‚úÖ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ö—É–∂–µ)
    df_sorted = df.sort_values('date')
    n = len(df_sorted)
    
    first_half = df_sorted.head(n // 2)
    second_half = df_sorted.tail(n // 2)
    
    corr_first = first_half['actual'].corr(first_half['pred_q50'])
    corr_second = second_half['actual'].corr(second_half['pred_q50'])
    
    print(f"\n   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è (–ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞): {corr_first:.3f}")
    print(f"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è (–≤—Ç–æ—Ä–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞): {corr_second:.3f}")
    
    if corr_first > corr_second + 0.15:
        print("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    else:
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏")
    
    # 7. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\n" + "=" * 70)
    print("üìã –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê")
    print("=" * 70)
    
    score = 0
    max_score = 5
    
    if coverage['calibration_error'] < 0.05:
        score += 1
        print("   [‚úì] –û—Ç–ª–∏—á–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –∫–≤–∞–Ω—Ç–∏–ª–µ–π")
    else:
        print("   [ ] –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
    
    if accuracy['correlation'] > 0.5:
        score += 1
        print("   [‚úì] –ü—Ä–∏–µ–º–ª–µ–º–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
    else:
        print("   [ ] –ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
    
    if abs(accuracy['bias']) < 0.02:
        score += 1
        print("   [‚úì] –ù–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è")
    else:
        print("   [ ] –ï—Å—Ç—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ")
    
    if accuracy['correlation'] < 0.95:
        score += 1
        print("   [‚úì] –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ data leakage")
    else:
        print("   [ ] –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ data leakage")
    
    if abs(corr_first - corr_second) < 0.15:
        score += 1
        print("   [‚úì] –ú–æ–¥–µ–ª—å —Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏")
    else:
        print("   [ ] –ú–æ–¥–µ–ª—å –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    print(f"\n   –ò–¢–û–ì–û: {score}/{max_score}")
    
    if score >= 4:
        print("   üéâ –ú–æ–¥–µ–ª—å –ì–û–¢–û–í–ê –∫ production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    elif score >= 3:
        print("   ‚úì –ú–æ–¥–µ–ª—å –ü–†–ò–ì–û–î–ù–ê —Å –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏")
    else:
        print("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¢–†–ï–ë–£–ï–¢ –î–û–†–ê–ë–û–¢–ö–ò")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    output_file = ML_ROOT / "reports" / "validation_detailed.csv"
    stats_df.to_csv(output_file, index=False)
    print(f"\n   üíæ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    
    return {
        'coverage': coverage,
        'accuracy': accuracy,
        'ticker_stats': stats_df
    }


if __name__ == "__main__":
    import io
    import sys
    
    # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    output_file = ML_ROOT / "reports" / "validation_console.txt"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º stdout
    original_stdout = sys.stdout
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Å–æ–ª—å
    with open(output_file, 'w', encoding='utf-8') as f:
        class Tee:
            def __init__(self, *files):
                self.files = files
            def write(self, data):
                for f in self.files:
                    f.write(data)
                    f.flush()
            def flush(self):
                for f in self.files:
                    f.flush()
        
        sys.stdout = Tee(original_stdout, f)
        
        try:
            run_validation()
        except Exception as e:
            print(f"ERROR: {e}")
            import traceback
            traceback.print_exc()
        finally:
            sys.stdout = original_stdout
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø–∏—Å–∞–Ω—ã –≤: {output_file}")

